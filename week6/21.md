Software Quality Engineering
============================

	Testing, Quality Assurance, and Quantiable Improvement

Tian Siyuan <tiansiyuan@gmail.com>

# Chapter 21. Risk Identification for Quantifiable Quality Improvement

-	Basic Ideas and Concepts
-	Traditional Statistical Techniques
-	Newer/More Effective Techniques
-	Tree-Based Analysis of ODC Data

# Risk Identification: Why?

-	Observations and empirical evidences:

	-	80:20 rule: non-uniform distribution:

		- 20% of the modules/parts/etc.

		  contribute to

		- 80% of the defects/effort/etc.

	-	implication: non-uniform attention

		- risk identification
		- risk management/resolution

-	Risk Identification in SQE:

	-	80:20 rule as implicit hypothesis
	-	focus: techniques and applications

# Risk Identification: How?

-	Qualitative and subjective techniques:

	-	Causal analysis
	-	Delphi and other subjective methods

-	Traditional statistical techniques:

	-	Correlation analysis
	-	Regression models:

		- linear, non-linear, logistic, etc.

-	Newer (more effective) techniques:

	-	Statistical: PCA, DA, TBM
	-	AI-based: NN, OSR
	-	Focus of our Chapter.

# Risk Identification: Where?

-	80% or target:

	-	Mostly quality or defect

		(most of our examples also)

	-	Effort and other external metrics
	-	Typically directly related to goal
	-	Resultant improvement

-	20% or contributor:

	-	20%: risk identification!
	-	Understand the link
	-	Control the contributor:

		- corrections/defect removal/etc.
		- future planning/improvement
		- remedial vs preventive actions

# Statistics Basics

-	Random variables

	-	used in statistical analysis/modeling and in risk identification in particular
	-	used as either i.v./p.v. or d.v./r.v.
	-	i.v. or p.v.: independent variable
		- also called predictor (variable)
	-	d.v. or r.v.: dependent variable
		- also called response (variable)

-	Statistical distributions:

	-	1d: normal, exponential, binomial, etc.
	-	2d: independent vs. correlated
	-	covariance, correlation (coefficient)

# Traditional Technique: Correlation

-	Correlation coefficient (c.c.):

	-	ranges between (cid:0)1 and 1
	-	positive: move in same direction
	-	negative: move in opposite direction
	-	0: not correlated (independent)

-	Correlation analysis:

	-	use correlation coefficient
	-	linear (Pearson) correlation vs. non-parametric (Spearman) correlation
	-	based on measurement type/distribution:

		- non-normal distribution
		- ordinal measurement etc.

# Traditional Technique: Correlation

-	Correlation analysis: applications

	-	understand general relationship

		- e.g., complexity-defect correlation

	-	risk identification also
	-	cross validation (metrics etc.)

-	Correlation analysis: assessment

	-	only partially successful
	-	low correlation, then what?
	-	data skew: 0-defect example
	-	uniform treatment of data

=> Other risk identification techniques needed.

# Traditional Technique: Regression

-	Regression models:

	-	as generalized correlation analysis
	-	n i.v. combined to predict 1 d.v.
	-	forms of prediction formula

		=> diff. types of regression models

-	Types of regression models:

	-	linear: linear function

		![](../pics/21-f-1.png)

	-	log-linear: linear after log-transformation
	-	non-linear: non-linear function
	-	logistic: represent presence/absence of categorical variables

# Traditional Technique: Regression

-	Regression analysis: applications

	-	similar to correlation analysis
	-	multiple attribute data

-	Regression analysis: assessment

	-	only partially successful
	-	similar to correlation analysis
	-	often marginally better (R-sqr vs c.c.)
	-	same kind of problems
	-	data transformation problem
	-	synthesized metrics ~ regression model?

=> Other risk identification techniques needed.

# New Techniques

-	New statistical techniques:

	-	PCA: principal component analysis
	-	DA: discriminant analysis
	-	TBM: tree-based modeling

-	AI-based new techniques:

	-	NN: artificial neural networks.
	-	OSR: optimal set reduction.
	-	Abductive-reasoning, etc.

-	Focus of our Chapter.

# New Techniques: PCA & DA

-	Not really new techniques, but rather new applications in SE.

-	PCA: principal component analysis

	-	Idea of linear transformation.
	-	PCA to reduce dimensionality.
	-	Effectively combined with DA and other techniques (NN later).

-	DA: discriminant analysis

	-	Discriminant function
	-	Risk id as a classification problem
	-	Combine with other techniques

# New Techniques: PCA & DA

-	PCA: why?

	-	Correlated i.v.’s => unstable models
	-	Extreme case: linearly dependent => singularity
	-	linear transformation (PCA) => uncorrelated PCs (or domain metrics)

-	PCA: how?

	-	Covariance matrix: (cid:6)
	-	Solve j(cid:6) (cid:0) (cid:3)j = 0 to obtain eigenvalues
(cid:21)j along the diagonal for the diagonal
matrix (cid:3)

	-	(cid:21)j’s in decreasing value
	-	Decomposition: (cid:6) = C T (cid:3)C
	-	C: matrix of eigenvectors

(transformation used)

# New Techniques: PCA & DA

-	Transformation to PCs/data matrix D:

	-	D = ZT , where
	-	Z is the original data matrix
	-	T is the transformation matrix

-	Obtaining PCA results: (cid:3); C; T can be

calculated by statistical packages/tools.

-	PCA result interpretation/usage:

	-	Eigenvalues (cid:25) explained variance.
	-	Uncorrelated PCs

) good/stable (linear/other) models

# New Techniques: PCA & DA

pc4
eigenvalue (cid:21)i
0.711
% of variance 55.3% 16.8% 10.8% 5.1%

pc3
1.042

pc1
2.352

pc2
1.296

cumulative

% of variance 55.3% 72.1% 82.9% 88.0%

-	PCA example: Table 21.1 (p.357)

	-	Eigenvalues sorted in decreasing order.
	-	First few (3-5) principal components (PCs)

explain most of the variance.

	-	Dimension reduction

) simplified models with a few PCs.

# New Techniques: PCA & DA

-	DA: general ideas

	-	Define discriminant function.
	-	Classify into G1 and G2

		- G1: not fault-prune
		- G2: fault-prune

-	Sample discriminant function:

	-	Assign di to G1, if

f1(di)
f2(di)

>

(cid:25)2
(cid:25)1

;

	-	Otherwise, assign di to G2.
	-	di:

i-th module’s principal-component
values (i-th row of the D matrix above).
	-	(cid:25)k: prior probability(membership in Gk).
	-	fk(di): probability(di 2 Gk).

# New Techniques: PCA & DA

-	DA: how?

	-	Discriminant function example above.
	-	Other/similar definitions possible.
	-	Minimize misclassification rate in model

fitting and in prediction.

	-	Good results (Khoshgoftaar et al., 1996).

-	PCA&DA: Summary and Observations:

	-	Positive/encouraging results, but,
	-	Much processing/transformation needed.
	-	Much statistics knowledge.
	-	Difficulty in data/result interpretation.

# New Technique: NN

-	NN or ANN: artificial neural networks

	-	Inspired by biological computation
	-	Neuron: basic computational unit

		- different functions

	-	Connection: neural network
	-	Input/output/hidden layers

-	NN applications:

	-	AI and AI problem solving
	-	In SQE: defect/risk identification

# New Technique: NN

-	Computation at a neuron:

	-	Weighted sum of input: h =

n
X
1

xi

(may include constant)

	-	Then activation function y = g(h)

		- threshold, piecewise-linear,
		- Gaussian, sigmoid (below), etc.

1

y =

1 + e(cid:0)fix

-	Illustration: Fig 21.1 (p.358)

w

1

2w

w

n

x

1

x

2

.
 
.
 
.

x

n

g(x)

h

y

x

# New Technique: NN

0. Initialize the weights to small random values.

1. Repeat steps 2 (cid:24) 6 until the error in the out-
put layer is below a pre-specified threshold or a
maximum number of iterations is reached.

2. Randomly choose an input.

3. Propagate the signal forward through the net-

work.

4. Compute the errors in the output layer.

5. Compute the deltas for the preceding layers by

propagating the errors backward.

6. Update the weights based on these deltas.

-	Overall NN computation: Fig 21.2 (p.359)

	-	input layer: raw data feed
	-	other layers: computation at n neurons
	-	minimize prediction error at the output

layer via backward propagation

# New Technique: NN

-	NN study (Khoshgoftaar and Szabo, 1996):

	-	train NN with Kernel.1 data

		- using both raw data and PCA data

	-	apply NN to Kernel.2 and Kernel.3

-	Result comparison: Table 21.2 (p.359)

	-	NN superior to linear regression.
	-	NN+PCA superior to NN on raw data.

System Model

data
Kernel.2 raw

PCA

Kernel.3 raw

PCA

Output error

mean std.dev min. max.
32.8
42.8
31.6
26.2

6.6 0.19
5.6 0.05
6.3 0.12
4.1 0.02

11.4
7.1
11.0
4.7

# New Technique: TBM

-	TBM: tree-based modeling

	-	Similar to decision trees
	-	But data-based (derived from data)
	-	Preserves tree advantages:

		- easy to understand/interpret
		- both numerical and categorical data
		- partition ) non-uniform treatment

-	TBM applications:

	-	Main: defect analysis

TBDMs (tree-based defect models)
	-	Past: psychology, SE-Amadeus, etc.
	-	Reliability: TBRMs (Ch.22)

-	Risk identification and characterization.

# New Technique: TBM

-	TBM for risk identification:

	-	Assumption in traditional techniques:

		- linear relation
		- uniformly valid result

	-	Reality of defect distribution:

		- isolated pocket
		- different types of metrics
		- correlation/dependency in metrics
		- qualitative differences

	-	Need new risk id. techniques.

-	TBM for risk characterization:

	-	Identified, then what?
	-	Result interpretation.
	-	Remedial/corrective actions.
	-	Extrapolation to new product/release.
	-	TBDMs appropriate.

# New Technique: TBM

-	TBDMs: tree-based defect models using

tree-based modeling (TBM) technique

-	Decision trees:

	-	multiple/multi-stage decisions
	-	may be context-sensitive
	-	natural to the decision process
	-	applications in many problems

		- decision making & problem solving
		- decision analysis/optimization

-	Tree-based models:

	-	reverse process of decision trees
	-	data ) tree
	-	idea of decision extraction
	-	generalization of \decision"

# New Technique: TBM

-	Technique: tree-based modeling

	-	Tree: nodes=data-set, edges=decision.
	-	Data attributes:

		- 1 response & n predictor variables.

	-	Construction: recursive partitioning

		- with tree growing and pruning.

-	TBM usage: relating r.v. to p.v.:

	-	Y = T ree(X1; : : : ; Xn)
	-	understanding vs. predicting
	-	identification and characterization

-	Types of data ) tree type

	-	numerical r.v.: regression tree
	-	categorical r.v.: classification tree

# TBM Algorithm: Fig 21.3 (p.360)

0.

Initialization. Set the list, Slist, to contain only the com-
plete data set as its singleton element. Select the size and
homogeneity thresholds Ts and Th for the algorithm.

1. Overall control. Repeatedly remove a data set from Slist

and execute step 2 until Slist becomes empty.

2. Size test.

If jSj < Ts, stop; otherwise, execute steps 3

through 6. jSj is the number of data points in set S.

3. Defining binary partitions. A binary partition divides S into
two subsets using a split condition defined on a specific pre-
dictor p. For numerical p, it can be defined with a cutoff
value c: Data points with p < c form one subset (S1) and
those with p (cid:21) c form another subset (S2).
If p is a cate-
gorical variable, a binary partition is a unique grouping of
all its category values into two mutually exclusive subsets
S1 and S2.

4. Computing predicted responses and prediction deviances.
The predicted response value v(S) for a set S is the av-
erage over the set; that is, v(S) = 1
(vi); and the
(vi (cid:0) v(S))2, where vi is
prediction deviance is D(S) = Pi2S
the response value for data point i.

jSj Pi2S

5. Selecting the optimal partition. Among all the possible
partitions (all predictors with all associated cutoffs or binary
groupings), the one that minimizes the deviance of the
partitioned subsets is selected; that is, the partition with
minimized D(S1) + D(S2) is selected.

6. Homogeneity test: Stop if (cid:0)1 (cid:0) D(S1)+D(S2)

(cid:1) (cid:20) Th (that is,
stop if there is no substantial gain in prediction accuracy in
further partitioning); otherwise, append S1 and S2 to Slist.

D(S)

# TBM Example

 DF=  7.9 
s.d.=  10.7 
    n= 995

HLSC<870.5

HLSC>870.5

 DF=  6.5 
 DF=  6.5 
s.d.=  6.8 
s.d.=  6.8 
    n= 944
    n= 944

 DF= 34.0 
 DF= 34.0 
s.d.=  27.1 
s.d.=  27.1 
    n= 51
    n= 51

HLSC<132.5

HLSC>132.5

HLSC<2450.5

HLSC>2450.5

 DF=  4.0 
s.d.=  3.2 
    n= 623

 DF= 11.0 
s.d.=  8.8 
    n= 321

 DF= 29.0 
 DF= 29.0 
s.d.=  21.0 
s.d.=  21.0 
    n= 46
    n= 46

 DF= 77.0 
s.d.= 35.8 
    n= 5

Legend: DF = mean DF
s.d.= std dev
 n = #modules

attr<cutoff

attr>cutoff

MLSC<263.7

MLSC>263.7

 DF= 35.0 
 DF= 35.0 
s.d.=  21.2 
s.d.=  21.2 
    n= 31
    n= 31

 DF= 17.0 
s.d.=  13.2 
    n= 15

McCabe<132

McCabe>132

 DF= 55.0 
s.d.=  17.5 
    n= 8

 DF= 28.0 
s.d.=  17.7 
    n= 23

-	TBDM example: Fig 21.4 (p.361)

	-	defect prediction for IBM-NS
	-	11 design/size/complexity metrics.

# TBM Example

Node: Split Conditions or

# of

Predicted

Subset Characteristics

modules

rlll: 870.5 < HLSC < 2450.5

^ MLSC < 263.7,
^ McCabe < 132

rr: HLSC > 2450:5

8

5

DF
55.0

77.0

-	Table 21.3 (p.361): high-risk subsets.

-	Design and control complexity as main

predictors to characterize high-risk.

-	Key \selling" points of TBM:

	-	intuitiveness and interpretation

		- compare to PCA, NN

	-	quantitative & qualitative info.
	-	hierarchy/importance/organization

# New Technique: OSR

-	OSR: optimal set reduction

	-	pattern matching idea
	-	clusters and cluster analysis
	-	similar to TBM but different in:

		- pattern extraction vs. partition

-	algorithm sketch: Fig 21.5 (p.362)

Step 1. Both the dependent (response) variable and
the explanatory (predictor or independent) vari-
ables are discretized by using cluster analysis or
some other method if they are continuous.

Step 2. Select all statistically significant subsets de-
fined by a pattern whose entropy (or uniformity)
is within a threshold of the minimal entropy.

Step 3. Step 2 is repeated until no significant gain

can be made in entropy reduction.

# New Technique: OSR

subset1

subset2

subset1

subset1.1

subset1.3

subset1.2

subset2

subset1.3

subset1.1

subset1.2

subset1.3

-	OSR illustration: Fig 21.6 (p.363)

-	Organization/modeling results:

	-	no longer a tree, see example above
	-	general subsets, may overlap
	-	details and some positive results:

see Briand et al. (1992)

# Risk Identification: Comparison

-	Comparison: cost-benefit analysis

(cid:25) comparing QA alternatives (Ch.17).

-	Comparison area: benefit-related

	-	accuracy
	-	early availability and stability
	-	constructive information and guidance

for (quality) improvement

-	Comparison area: cost-related

	-	simplicity
	-	ease of result interpretation
	-	availability of tool support

# Comparison: Accuracy

-	Accuracy in assessment:

	-	model fits data well

		- use various goodness-of-fit measures

	-	avoid over-fitting
	-	cross validation by review etc.

-	Accuracy in prediction:

	-	over-fitting ) bad predictions
	-	prediction: training and testing sets

		- within project: jackknife
		- across projects: extrapolate

	-	minimize prediction errors

# Comparison: Usefulness

-	Early availability and stability

	-	to be useful must be available early
	-	focus on control/improvement
	-	apply remedial/preventive actions early
	-	track progress: stability

-	constructive information and guidance

	-	what: assessment/prediction
	-	how to improve?

		- constructive information
		- guidance on what to do

	-	example of TBRMs

# Comparison: Usability

-	Can’t explain in a few words

	=> difficulties with reception/deployment

-	Simplicity & result interpretation?

	-	technique easy to use/understand
	-	what does it (the result) mean?
	-	training effort involved
	-	causal and other connections

-	Tool and other support:

	-	availability of easy-to-use tools
	-	other support: process/personnel/etc.
	-	direct impact on deployment

# Comparison Summary

(Table 21.4)

Technique

correlation
regression
PCA/DA
NN
TBM
OSR

Benefit/Performance

accuracy

stability

guidance

poor
poor
good
good
good
good

fair
poor

excellent

fair
good
fair

fair
poor
fair
poor

excellent
excellent

(Table 21.4 continued)

Technique

Cost/Usability

simplicity
simplest
simple

interp.
easiest

moderate

tool sup.

correlation
regression
PCA/DA moderate moderate moderate
NN
moderate
TBM
moderate
OSR

complex
moderate
complex moderate

wide
wide

hard
easy

limited

-	Summary: Table 21.4 (p.364)

# Recommendations and Integration

-	General recommendations:

	-	TBM good balance ) prime candidate.
	-	Other techniques might be useful too.
	-	A suite of techniques might be suitable.

-	Lifecycle integration:

	-	Process and data availability

) inspection/testing/other QA data.

	-	Experience/infrastructure/tools/etc. for

implementation/technology transfer.
	-	Similar techniques for other problems

		- e.g., identifying effort, schedule risks.
	-	Tailoring to individual process/product

# Tree-Based ODC Data Analysis

-	Continuation of ODC analysis:

	-	IBM Toronto data from ODC (Ch.20)
	-	1-way ! 2-way ! n-way analyses

		- combinatorial explosion

	-	Better focus on n-1 linkage:

		- 1 response variable: impact
		- n (=6 here) predictor variables

	-	ODC attributes in Table 20.6 (p.347)

		- all except \severity" used
		- impact-severity analysis already done:

see Table 20.7 (p.351)

-	Tree-based ODC modeling

	-	Classification trees

(instead of regression trees)

	-	Change in distribution

# Tree-Based ODC Data Analysis

root
reliab
0.51
438

v=
 f=
d=

trig: e,a,,b,f,i,o,sc      trig: c,co,m,ss,s

l

v=
 f=
d=

reliab
0.32
256

f

r

v=
 f=
d=

reliab
0.775
182

trig: a,i        trig: e,b,f,o,sc

wk < 216    wk > 216

ll
v=
 f=
d=

reliab
0.316
76

lr
v=
 f=
d=

reliab
0.322
180

rl
v=
 f=
d=

reliab
0.5
16

rr
v=
 f=
d=

reliab
0.801
166

inj: p,c,lld,ft,st    inj:hld,ut
lll
v=
 f=
d=

reliab
0.583
12

usabi
0.375
64

llr
v=
 f=
d=

wk < 231.5   wk > 231.5

lrl
v=
 f=
d=

capab
0.2
55

lrr
v=
 f=
d=

reliab
0.376
125

src: n,s    src: b,c,i,p,v

rrl

v=
 f=
d=

reliab
0.536
41

rrr
v=
 f=
d=

reliab
0.888
125

src: c,i,s    src: b,n,p,v

llll

v=
 f=
d=

servi
0.333
12

lllr

v=
 f=
d=

usabi
0.442
52

wk < 248.5    wk > 248.5
lrrl
v=
 f=
d=

docum
0.6
5

reliab
0.392
120

lrrr
v=
 f=
d=

inj: lld, ft    inj: p,c,st

lllrl

v=
 f=
d=

secur
0.5
6

lllrr
v=
 f=
d=

usabi
0.478
46

node label

v= predicted impact
 f= frequency of v
d= number of defects

Legend:

-	Overall result: Fig 21.7 (p.366)

		- dominant impact shown in tree nodes.

# Tree-Based ODC Data Analysis

u

 
,

d

t
s
 
,
r
e
s
 
,
c
e
s
 
,
r
 
,
p
 
,
i

m

 
,
a
m

 
,
n
i
 
,

m

i
 
,
c
 
 
:
e
c
n
e
u
q
e
s
 
e
d
a
h
s
 
r
a
B

0
0
4

0
0
3

0
0
2

0
0
1

0

All 438 defects

USABILITY

256 defects; trig: e,a,b,f,i,o,sc

USABILITY

RELIABILITY

182 defects; trig: c,co,m,ss,s

RELIABILITY

RELIABILITY

CAPABILITY

CAPABILITY

LEFT SUBTREE

ROOT TREE

RIGHT SUBTREE

-	Fig 21.8 (p.367): impact distribution

	-	Primary partition: defect trigger
	-	High homogeneity of right subtree
	-	Problem identification: left subtree

# Tree-Based ODC Data Analysis

-	Result interpretation:

	-	Overall result: Fig 21.7 (p.366)
	-	Dominant impact: tree nodes.
	-	Impact distribution: bars.
	-	Confidence: frequency and cardinality.

-	Usage of modeling results:

	-	Passive tracking and correction
	-	Active problem identification and quality

control

-	Other ODC trees possible

(for different r.v.)
